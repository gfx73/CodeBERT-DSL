{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\nCOMET_API_KEY = secrets.get_secret('comet_api_key')","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:03:35.324972Z","iopub.execute_input":"2023-06-01T16:03:35.325366Z","iopub.status.idle":"2023-06-01T16:03:35.580855Z","shell.execute_reply.started":"2023-06-01T16:03:35.325321Z","shell.execute_reply":"2023-06-01T16:03:35.579897Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport random\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nFLAGS = {\n    # batch size used in model\n    'batch_size': 12,\n    # batch size for metrics calculaation\n    'batch_size_for_knn': 256,\n    # max epochs for training\n    'max_epochs': 3,\n    # to set manual seed, random if None\n    'SEED': None,\n    # one of {'train', 'valid', 'test'}\n    'stage': 'test',\n    # to use checkpoint, set None to use defeault CodeBERT\n    'CHECKPOINT_PATH': '/kaggle/input/final-weights/semihard+hard_checkpoints/epoch=2-step=24471.ckpt',\n    # name for logger experiment\n    'EXPERIMENT_NAME': \"semihard+hard_test\",\n    # name for logger project\n    'PROJECT_NAME': 'typebert4py',\n    # to follow more simple precossing steps\n    'preprocess_as_hityper': False,\n    # to do fast run with 10% of data\n    'trial_run': False\n}\n\nMODEL_PARAMS = {\n    # margin value\n    'margin': 2,\n    # number of infered types\n    'number_of_neighbors_to_find': 10,\n    # learning rate\n    'lr': 1e-5,\n    # either HARD or SEMI_HARD\n    'loss_type': 'HARD',\n    # to use same parameters as in checkpoint\n    'keep_same': True,\n    # embedding dimensionality the model produces. CodeBERT specific\n    'emb_dim': 768,\n    # number of tokens model can accept. CodeBERT specific\n    'model_max_length': 512\n}\n\n\n# dataset directory\nDATASET_DIR = Path('/kaggle/input/manytypes4pyaggregated')\nTRAIN_PATH = DATASET_DIR / 'train.csv'\nTEST_PATH = DATASET_DIR / 'test.csv'\nVALID_PATH = DATASET_DIR / 'valid.csv'\n\nif not FLAGS['SEED']:\n    FLAGS['SEED'] = random.randint(1, 1000)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:03:35.584604Z","iopub.execute_input":"2023-06-01T16:03:35.584895Z","iopub.status.idle":"2023-06-01T16:03:35.593565Z","shell.execute_reply.started":"2023-06-01T16:03:35.584870Z","shell.execute_reply":"2023-06-01T16:03:35.592505Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-metric-learning -q\n!pip install comet-ml -q\n!pip install faiss-gpu -q","metadata":{"_uuid":"0765fd20-4300-4f97-895d-55604b253540","_cell_guid":"cba38665-223d-46de-af00-56bbc4e4e9c2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-01T16:03:35.594902Z","iopub.execute_input":"2023-06-01T16:03:35.595317Z","iopub.status.idle":"2023-06-01T16:04:09.390315Z","shell.execute_reply.started":"2023-06-01T16:03:35.595282Z","shell.execute_reply":"2023-06-01T16:04:09.389149Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nfrom pytorch_metric_learning import miners, losses\nfrom pytorch_metric_learning.utils import accuracy_calculator\n\nfrom tokenizers import processors\nfrom transformers import AutoTokenizer, AutoModel\nfrom datasets import load_dataset\n\nimport faiss\nimport faiss.contrib.torch_utils\n\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import TQDMProgressBar, EarlyStopping, ModelCheckpoint, Callback\nfrom pytorch_lightning.strategies import DDPStrategy\nfrom pytorch_lightning.loggers import CometLogger\n\n\nfrom torchmetrics import MeanMetric\n\ntry:\n   import ipywidgets\n   from tqdm.auto import tqdm\nexcept ImportError as e:\n   from tqdm import tqdm\n\nfrom collections import defaultdict\nfrom functools import partial\nfrom ast import literal_eval\nfrom enum import Enum, auto\nimport gc\nimport math\nfrom collections import Counter\nimport regex","metadata":{"_uuid":"68142d6a-7fb4-4e7c-95cf-b9700f99a8d9","_cell_guid":"4afd1b97-57ae-4c8e-9091-43b4d90d5fcd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-01T16:04:09.394530Z","iopub.execute_input":"2023-06-01T16:04:09.394870Z","iopub.status.idle":"2023-06-01T16:04:09.405086Z","shell.execute_reply.started":"2023-06-01T16:04:09.394834Z","shell.execute_reply":"2023-06-01T16:04:09.404095Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything(FLAGS['SEED'])","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:04:09.407345Z","iopub.execute_input":"2023-06-01T16:04:09.407718Z","iopub.status.idle":"2023-06-01T16:04:09.428398Z","shell.execute_reply.started":"2023-06-01T16:04:09.407684Z","shell.execute_reply":"2023-06-01T16:04:09.427411Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"983"},"metadata":{}}]},{"cell_type":"code","source":"# calculate the number of types before preprocessing\n\n\n# data = load_dataset(\"csv\", data_files={\"train\": str(TRAIN_PATH), \"test\": str(TEST_PATH), \"valid\": str(VALID_PATH)})\n# train_data = data['train'] #.train_test_split(test_size=1000, shuffle=False)['test']\n# valid_data = data['valid'] #.train_test_split(test_size=1000, shuffle=False)['test']\n# test_data = data['test'] #.train_test_split(test_size=1000, shuffle=False)['test']\n\n# annotations_num_train = 0\n# types_train = set()\n# for ex in tqdm(train_data):\n#     typed_seq = literal_eval(ex['typed_seq'])\n#     annotations_num_train += sum(map(lambda type_: type_ != '0', typed_seq))\n#     types_train.update(typed_seq)\n\n# annotations_num_valid = 0\n# types_valid = set()\n# for ex in tqdm(valid_data):\n#     typed_seq = literal_eval(ex['typed_seq'])\n#     annotations_num_valid += sum(map(lambda type_: type_ != '0', typed_seq))\n#     types_valid.update(typed_seq)\n    \n# annotations_num_test = 0\n# types_test = set()\n# for ex in tqdm(test_data):\n#     typed_seq = literal_eval(ex['typed_seq'])\n#     annotations_num_test += sum(map(lambda type_: type_ != '0', typed_seq))\n#     types_test.update(typed_seq)\n\n# print('train annotations:', annotations_num_train)\n# print('train types num:', len(types_train) - 1)\n\n# print('valid annotations:', annotations_num_valid)\n# print('valid types num:', len(types_valid) - 1)\n\n# print('test annotations:', annotations_num_test)\n# print('test types num:', len(types_test) - 1)\n\n# all_types = set()\n# all_types.update(types_train)\n# all_types.update(types_valid)\n# all_types.update(types_test)\n# print(len(all_types)-1)\n# print(annotations_num_train+annotations_num_valid+annotations_num_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:04:09.430119Z","iopub.execute_input":"2023-06-01T16:04:09.430464Z","iopub.status.idle":"2023-06-01T16:04:09.438251Z","shell.execute_reply.started":"2023-06-01T16:04:09.430432Z","shell.execute_reply":"2023-06-01T16:04:09.437348Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"SUB_REGEX = r'typing\\.|typing_extensions\\.|t\\.|builtins\\.|collections\\.'\n\nTYPE_ALIASES = {'(?<=.*)any(?<=.*)|(?<=.*)unknown(?<=.*)': 'Any',\n                '^{}$|^Dict$|^Dict\\[\\]$|(?<=.*)Dict\\[Any, *?Any\\](?=.*)|^Dict\\[unknown, *Any\\]$': 'dict',\n                '^Set$|(?<=.*)Set\\[\\](?<=.*)|^Set\\[Any\\]$': 'set',\n                '^Tuple$|(?<=.*)Tuple\\[\\](?<=.*)|^Tuple\\[Any\\]$|(?<=.*)Tuple\\[Any, *?\\.\\.\\.\\](?=.*)|^Tuple\\[unknown, *?unknown\\]$|^Tuple\\[unknown, *?Any\\]$|(?<=.*)tuple\\[\\](?<=.*)': 'tuple',\n                '^Tuple\\[(.+), *?\\.\\.\\.\\]$': r'Tuple[\\1]',\n                '\\\\bText\\\\b': 'str',\n                '^\\[\\]$|(?<=.*)List\\[\\](?<=.*)|^List\\[Any\\]$|^List$': 'list',\n                '^\\[{}\\]$': 'List[dict]',\n                '(?<=.*)Literal\\[\\'.*?\\'\\](?=.*)': 'Literal',\n                '(?<=.*)Literal\\[\\d+\\](?=.*)': 'Literal',\n                '^Callable\\[\\.\\.\\., *?Any\\]$|^Callable\\[\\[Any\\], *?Any\\]$|^Callable[[Named(x, Any)], Any]$': 'Callable',\n                '^Iterator[Any]$': 'Iterator',\n                '^OrderedDict[Any, *?Any]$': 'OrderedDict',\n                '^Counter[Any]$': 'Counter',\n                '(?<=.*)Match[Any](?<=.*)': 'Match'}\n\nEXCLUDE_TYPES = ['Any', 'None', 'object', 'type', 'Type[Any]',\n                    'Type[cls]', 'Type[type]', 'Type', 'TypeVar', 'Optional[Any]']\n\nUBIQUITOUS_TYPES = {'str', 'int', 'list', 'bool', 'float'}\nUBIQUITOUS_TYPE_IDS = set(range(len(UBIQUITOUS_TYPES)))\nUBIQUITOUS_TYPE2ID = {k: v for k, v in zip(UBIQUITOUS_TYPES, UBIQUITOUS_TYPE_IDS)}\n\n# the tokenized code is stored as str, however it is a list. Here the convertation is done \ndef batch_str_to_list(batch):\n    batch['untyped_seq'] = [literal_eval(seq) for seq in batch['untyped_seq']]\n    batch['typed_seq'] = [literal_eval(seq) for seq in batch['typed_seq']]\n    return batch\n\n# removing undesired tokens, replacing [EOL] by \\n\ndef preprocess_normalized_seq2seq(untyped_seq, typed_seq):\n    new_untyped, new_typed = [], []\n    for untyped, typed in zip(untyped_seq, typed_seq):\n        if untyped == '[EOL]':\n            if len(new_untyped) != 0:\n                if new_untyped[-1][-1] != '\\n':\n                    new_untyped[-1] += '\\n'\n        elif untyped == '[docstring]' or untyped == '[comment]':\n            continue\n        else:\n            new_untyped.append(untyped)\n            new_typed.append(typed)\n    return new_untyped, new_typed\n \n# preprocess_normalized_seq2seq for batches\ndef batch_preprocess_normalized_seq2seq(batch):\n    new_untyped_batch, new_typed_batch = [], []\n    for untyped_seq, typed_seq in zip(batch['untyped_seq'], batch['typed_seq']):\n        new_untyped, new_typed = preprocess_normalized_seq2seq(untyped_seq, typed_seq)\n        new_untyped_batch.append(new_untyped)\n        new_typed_batch.append(new_typed)\n    batch['untyped_seq'] = new_untyped_batch\n    batch['typed_seq'] = new_typed_batch\n    return batch\n\n\ndef preprocess_types_as_hitiper(typed_seq):\n    new_seq = []\n    for t in typed_seq:\n        if t == '$typing.Any$':\n            new_seq.append('0')\n        else:\n            new_seq.append(t)\n    return new_seq\n\ndef batch_preprocess_types_as_hitiper(batch):\n    batch['typed_seq'] = [preprocess_types_as_hitiper(typed_seq) for typed_seq in batch['typed_seq']]\n    return batch\n\n# make one type consistent\ndef make_consistent(t):\n    if t == '0':\n        return t\n    return regex.sub(SUB_REGEX, \"\", t[1:-1])\n\n# make consistent an example from dataset\ndef ex_make_consistent(ex):\n    ex['typed_seq'] = ['0' if t == '0' else make_consistent(t) for t in ex['typed_seq']]\n    return ex\n\ndef remove_quote_types(t: str):\n    if t == '0':\n        return t\n    s = regex.search(r'^\\'(.+)\\'$', t)\n    if bool(s):\n        return s.group(1)\n    else:\n        return t\n    \ndef ex_remove_quote_types(ex):\n    ex['typed_seq'] = ['0' if t == '0' else remove_quote_types(t) for t in ex['typed_seq']]\n    return ex\n\ndef exclude_types(t):\n    return '0' if t in EXCLUDE_TYPES else t\n\ndef ex_exclude_types(ex):\n    ex['typed_seq'] = ['0' if t == '0' else exclude_types(t) for t in ex['typed_seq']]\n    return ex\n\ndef resolve_type_alias(t: str):\n    if t == '0':\n        return t  \n    for t_alias in TYPE_ALIASES:\n        if regex.search(regex.compile(t_alias), t):\n            t = regex.sub(regex.compile(t_alias), TYPE_ALIASES[t_alias], t)\n    return t\n\ndef ex_resolve_type_alias(ex):\n    ex['typed_seq'] = ['0' if t == '0' else resolve_type_alias(t) for t in ex['typed_seq']]\n    return ex\n\n# reducing parametric types\ndef reduce_parameters(t):\n    nested_level = 1\n    new_t = ''\n    for c in t:\n        if c == '[':\n            nested_level += 1\n            if nested_level == 3:\n                new_t += '[Any]'\n                \n        if c == ']':\n            nested_level -= 1\n            if nested_level == 2:\n                continue\n        \n        if nested_level > 2:\n            continue\n        else:\n            new_t += c\n    return new_t\n\ndef ex_reduce_parameters(ex):\n    ex['typed_seq'] = ['0' if t == '0' else reduce_parameters(t) for t in ex['typed_seq']]\n    return ex\n\ndef remove_trivial_annotations(untyped_seq, typed_seq):\n    new_seq = []\n    for token, t in zip(untyped_seq, typed_seq):\n        if t == '__len__' or t == '__str__':\n            new_seq.append('0')\n        else:\n            new_seq.append(t)\n    return new_seq\n\ndef batch_remove_trivial_annotations(batch):\n    batch['typed_seq'] = [remove_trivial_annotations(untyped_seq, typed_seq) for untyped_seq, typed_seq in zip(batch['untyped_seq'], batch['typed_seq'])]\n    return batch\n\ndef ex_has_type(ex):\n    return any(map(lambda t: t != '0', ex['typed_seq']))\n\n\nclass SeqType(Enum):\n    INPUT_IDS = auto()\n    ATTENTION_MASK = auto()\n    TYPED_SEQ = auto()\n\n\nclass TokenizeTransform(object):\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.type2id = defaultdict(lambda: len(self.type2id))\n        self.type2id['0'] = -100\n        self.type2id['-100'] = -100\n        self.type2id.update(UBIQUITOUS_TYPE2ID)\n        \n\n    def __call__(self, batch):\n        return self.create_one_batch(batch)\n    \n    def get_id2type(self):\n        return {v: k for k, v in self.type2id.items()}\n\n    def create_one_batch(self, batch):\n        batch_encoding = self.tokenizer(\n            text = [['']] * len(batch['untyped_seq']),\n            text_pair = batch['untyped_seq'],\n            add_special_tokens=False,\n            truncation = False,\n            padding = False,\n            return_token_type_ids=True,\n            is_split_into_words=True\n        )\n\n        \n        input_ids_batch = batch_encoding.input_ids        \n        attention_mask_batch = batch_encoding.attention_mask\n        \n        typed_seq_batch = TokenizeTransform.align_typed_seq(batch_encoding, batch)\n        typed_seq_batch_wo_t_params = TokenizeTransform.batch_remove_type_parameters(typed_seq_batch)\n        typed_seq_batch = self.assign_ids_to_typed_seq_batch(typed_seq_batch)\n        typed_seq_batch_wo_t_params = self.assign_ids_to_typed_seq_batch(typed_seq_batch_wo_t_params)\n\n        input_ids_batch = self.batch_split_to_chunks_pad_add_special_tokens(input_ids_batch, SeqType.INPUT_IDS)\n        attention_mask_batch = self.batch_split_to_chunks_pad_add_special_tokens(attention_mask_batch, SeqType.ATTENTION_MASK)\n        typed_seq_batch = self.batch_split_to_chunks_pad_add_special_tokens(typed_seq_batch, SeqType.TYPED_SEQ)\n        typed_seq_batch_wo_t_params = self.batch_split_to_chunks_pad_add_special_tokens(typed_seq_batch_wo_t_params, SeqType.TYPED_SEQ)\n        \n        return {'input_ids_batch': input_ids_batch,\n                'attention_mask_batch': attention_mask_batch,\n                'typed_seq_batch': typed_seq_batch,\n                'typed_seq_batch_wo_t_params': typed_seq_batch_wo_t_params\n               }\n    \n    def align_typed_seq(batch_encoding, batch):\n        typed_seq_batch = []\n        for batch_idx, typed_seq in enumerate(batch['typed_seq']):\n            word_ids = batch_encoding.word_ids(batch_idx)\n            previous_word_idx = None\n            labels = []\n            for word_idx in word_ids:\n                if word_idx is None or word_idx == previous_word_idx:\n                    labels.append('-100')\n                else:\n                    labels.append(typed_seq[word_idx])\n                previous_word_idx = word_idx\n            \n            typed_seq_batch.append(labels)\n            \n        return typed_seq_batch\n    \n    \n    def assign_ids_to_typed_seq_batch(self, typed_seq_batch):\n        return list(map(\n            lambda typed_seq: list(map(\n                lambda type_: self.type2id[type_],\n                typed_seq)),\n            typed_seq_batch)) \n    \n    def remove_type_parameters(typed_seq):\n        new_typed_seq = []\n        for type_ in typed_seq:\n            beg = type_.find('[')\n            if beg == -1:\n                new_typed_seq.append(type_)\n            else:\n                new_typed_seq.append(type_[:beg])\n        return new_typed_seq\n        \n    def batch_remove_type_parameters(typed_seq_batch):\n        return [TokenizeTransform.remove_type_parameters(typed_seq) for typed_seq in typed_seq_batch]\n    \n    def batch_split_to_chunks_pad_add_special_tokens(self, batch, seq_type: SeqType):\n        chunkenized_batch = []\n        for lst in batch:\n            chunkenized_batch += self.split_to_chunks_pad_add_special_tokens(lst, seq_type)\n        return chunkenized_batch\n        \n    def split_to_chunks_pad_add_special_tokens(self, lst, seq_type: SeqType):\n        if seq_type == SeqType.INPUT_IDS:\n            padding_token = self.tokenizer.pad_token_id\n        elif seq_type == SeqType.ATTENTION_MASK:\n            padding_token = 0\n        else:\n            padding_token = -100\n            \n        chunks = []\n        chunk_len = self.tokenizer.model_max_length - 3\n        # dividing to chunks\n        for i in range(0, len(lst), chunk_len):\n            el = lst[i:i + chunk_len]\n            # adding special tokens\n            el = self.add_special_tokens(el, seq_type)\n            \n            # padding\n            if len(el) != self.tokenizer.model_max_length:\n                el += [padding_token] * (self.tokenizer.model_max_length - len(el))\n            chunks.append(el)\n        return chunks\n    \n    def add_special_tokens(self, lst, seq_type: SeqType):        \n        if seq_type == SeqType.INPUT_IDS:\n            lst = [self.tokenizer.cls_token_id] + [self.tokenizer.sep_token_id] + lst + [self.tokenizer.eos_token_id]\n        elif seq_type == SeqType.ATTENTION_MASK:\n            lst = [1] + [1] + lst + [1]\n        else:\n            lst = [-100] + [-100] + lst + [-100]\n        return lst\n    \n\nclass PreprocessData:\n    def __init__(self, tokenize_transform):\n        self.tokenize_transform = tokenize_transform\n    \n    def get_id2type(self):\n        return self.tokenize_transform.get_id2type()\n    \n    def get_type2id(self):\n        return self.tokenize_transform.type2id\n    \n    def __call__(self, data):\n        # doing all preprocessing\n        if FLAGS['preprocess_as_hityper']:\n            return data\\\n                .map(batch_str_to_list, batched=True, desc=\"Strings to lists\")\\\n                .filter(lambda example: len(example['untyped_seq']) == len(example['typed_seq']), desc='Removing examples with unuequal sequences len')\\\n                .map(batch_preprocess_normalized_seq2seq, batched=True, desc='Preprocessing normalizedseq2seq')\\\n                .map(batch_preprocess_types_as_hitiper, batched=True, desc='Preprocessing types',)\\\n                .map(self.tokenize_transform, batched=True, remove_columns=data.column_names, batch_size=FLAGS['batch_size'], desc='Tokenizing for the model').with_format(\"torch\")\\\n                .filter(lambda example: any(example['typed_seq_batch'] != -100), desc='Removing examples with no annotations')\n        else:\n            return data\\\n                .map(batch_str_to_list, batched=True, desc=\"Strings to lists\")\\\n                .filter(lambda example: len(example['untyped_seq']) == len(example['typed_seq']), desc='Removing examples with unuequal sequences len')\\\n                .map(batch_preprocess_normalized_seq2seq, batched=True, desc='Preprocessing normalizedseq2seq')\\\n                .map(ex_make_consistent, desc='Making consistent')\\\n                .map(ex_remove_quote_types, desc='Removing quote types')\\\n                .map(ex_exclude_types, desc='Excluding types')\\\n                .map(ex_resolve_type_alias, desc='Resolving type alias')\\\n                .map(ex_reduce_parameters, desc='Reducing parameters')\\\n                .map(batch_remove_trivial_annotations, batched=True, desc='Removing trivial annotations')\\\n                .filter(lambda ex: ex_has_type(ex), desc='Removing examples with no annotations')\\\n                .map(self.tokenize_transform, batched=True, remove_columns=data.column_names, batch_size=FLAGS['batch_size'], desc='Tokenizing for the model').with_format(\"torch\")\\\n                .filter(lambda example: any(example['typed_seq_batch'] != -100), desc='Removing examples with no annotations')\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\", add_prefix_space=True)\ntokenizer._tokenizer.post_processor = processors.BertProcessing(\n    sep=(\"</s>\", tokenizer._tokenizer.token_to_id(\"</s>\")),\n    cls=(\"<s>\", tokenizer._tokenizer.token_to_id(\"<s>\"))\n)\nnew_tokens = ['[number]', '[string]']\nnew_tokens = set(new_tokens) - set(tokenizer.vocab.keys())\ntokenizer.add_tokens(list(new_tokens))\n\npreprocess_data = PreprocessData(TokenizeTransform(tokenizer))","metadata":{"_uuid":"ae6a2b48-e73e-4ad4-8b83-3d5736e55535","_cell_guid":"43119d21-1d24-418c-8698-f334d4fa2d2c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-01T16:04:09.441470Z","iopub.execute_input":"2023-06-01T16:04:09.441759Z","iopub.status.idle":"2023-06-01T16:04:09.852754Z","shell.execute_reply.started":"2023-06-01T16:04:09.441735Z","shell.execute_reply":"2023-06-01T16:04:09.851768Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# dataset\nclass ManyTypes4PyDataModule(pl.LightningDataModule):\n    def __init__(self, train_path, test_path, valid_path, trial_run):\n        super().__init__()\n        self.train_path = train_path\n        self.test_path = test_path\n        self.valid_path = valid_path\n        self.trial_run = trial_run\n\n\n    def setup(self, stage: str):\n        data = load_dataset(\"csv\", data_files={\"train\": str(self.train_path), \"test\": str(self.test_path), \"valid\": str(self.valid_path)})\n        if self.trial_run:\n            train_data = data['train'].train_test_split(test_size=0.1, shuffle=False)['test']\n            valid_data = data['valid'].train_test_split(test_size=0.1, shuffle=False)['test']\n            test_data = data['test'].train_test_split(test_size=0.1, shuffle=False)['test']\n        else:\n            train_data = data['train']\n            valid_data = data['valid']\n            test_data = data['test']\n        \n        \n        if stage == \"fit\":\n            self.train_dataset = preprocess_data(train_data)\n            self.valid_dataset = preprocess_data(valid_data)\n            \n        if stage == \"test\":\n            if not hasattr(self, 'train_dataset'):\n                self.train_dataset = preprocess_data(train_data)\n            self.test_dataset = preprocess_data(test_data)\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=FLAGS['batch_size'],\n            drop_last=True,\n            shuffle=True\n        )\n\n    def val_dataloader(self):\n        dataloaders = [\n            torch.utils.data.DataLoader(\n                self.train_dataset,\n                batch_size=FLAGS['batch_size'],\n                drop_last=True\n            ),\n            torch.utils.data.DataLoader(\n                self.valid_dataset,\n                batch_size=FLAGS['batch_size'],\n                drop_last=True\n            )\n            \n        ]\n        return dataloaders\n\n    def test_dataloader(self):\n        dataloaders = [\n            torch.utils.data.DataLoader(\n                self.train_dataset,\n                batch_size=FLAGS['batch_size'],\n                drop_last=True\n            ),\n            torch.utils.data.DataLoader(\n                self.test_dataset,\n                batch_size=FLAGS['batch_size'],\n                drop_last=True\n            )\n            \n        ]\n        return dataloaders\n\ndm = ManyTypes4PyDataModule(train_path=TRAIN_PATH, test_path=TEST_PATH, valid_path=VALID_PATH, trial_run=FLAGS['trial_run'])","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:04:09.854304Z","iopub.execute_input":"2023-06-01T16:04:09.854670Z","iopub.status.idle":"2023-06-01T16:04:09.881643Z","shell.execute_reply.started":"2023-06-01T16:04:09.854635Z","shell.execute_reply":"2023-06-01T16:04:09.880520Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def get_types_count(types: torch.Tensor):\n    return Counter(types.cpu().numpy())\n\ndef get_is_ubiquitous_mask(types: torch.Tensor, ubiquitous_types: set):\n    return torch.tensor([t.item() in ubiquitous_types for t in types], device=types.device)\n\ndef get_is_common_mask(types: torch.Tensor, types_count: Counter, ubiquitous_types: set):\n    return torch.tensor([types_count[t.item()] > 100 and t.item() not in ubiquitous_types for t in types], device=types.device)\n\ndef get_is_rare_mask(types: torch.Tensor, types_count: Counter):\n    return torch.tensor([types_count[t.item()] <= 100 for t in types], device=types.device)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:04:09.884417Z","iopub.execute_input":"2023-06-01T16:04:09.884827Z","iopub.status.idle":"2023-06-01T16:04:09.893335Z","shell.execute_reply.started":"2023-06-01T16:04:09.884792Z","shell.execute_reply":"2023-06-01T16:04:09.892366Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# calculate the number of types after preprocessing\n\n\n# def count_types(train_dl):\n#     types = Counter()\n#     for batch in tqdm(train_dl):\n#         typed_seq_batch = batch['typed_seq_batch']\n#         has_type_annotation_mask = typed_seq_batch != -100\n#         type_ids = typed_seq_batch[has_type_annotation_mask]\n\n#         types.update(type_ids.flatten().tolist())\n\n    \n#     return types\n\n\n# def get_stats(dl, types_count):\n#     types = []\n#     for batch in tqdm(dl):\n#         typed_seq_batch = batch['typed_seq_batch']\n#         has_type_annotation_mask = typed_seq_batch != -100\n#         type_ids = typed_seq_batch[has_type_annotation_mask]\n\n#         types += type_ids.flatten().tolist()\n        \n    \n#     ubiquitous = [t for t in types if t in UBIQUITOUS_TYPE_IDS]\n#     common = [t for t in types if types_count[t] > 100 and t not in UBIQUITOUS_TYPE_IDS]\n#     rare = [t for t in types if types_count[t] <= 100]\n\n#     all_num = len(types)\n#     all_unique = len(set(types))\n    \n#     ubiquitous_num = len(ubiquitous)\n#     ubiquitous_unique = len(set(ubiquitous))\n    \n#     common_num = len(common)\n#     common_unique = len(set(common))\n    \n#     rare_num = len(rare)\n#     rare_unique = len(set(rare))\n    \n#     return {\n#             'all_num': all_num,\n#             'all_unique': all_unique,\n#             'ubiquitous_num': ubiquitous_num,\n#             'ubiquitous_unique': ubiquitous_unique,\n#             'common_num': common_num,\n#             'common_unique': common_unique,\n#             'rare_num': rare_num,\n#             'rare_unique': rare_unique\n#         }, set(types)\n\n\n# dm.setup('fit')\n# types_count = count_types(dm.train_dataloader())\n\n\n# train_stats, train_types = get_stats(dm.train_dataloader(), types_count)\n\n# valid_stats, valid_types = get_stats(dm.val_dataloader()[1], types_count)\n\n# dm.setup('test')\n# test_stats, test_types = get_stats(dm.test_dataloader()[1], types_count)\n\n\n# overall_unique = set()\n# overall_unique.update(train_types)\n# overall_unique.update(valid_types)\n# overall_unique.update(test_types)\n\n# print('Overall unique:', len(overall_unique))\n# print('train stats:', train_stats)\n# print('valid stats:', valid_stats)\n# print('test stats:', test_stats)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:04:09.896556Z","iopub.execute_input":"2023-06-01T16:04:09.896948Z","iopub.status.idle":"2023-06-01T17:05:56.107927Z","shell.execute_reply.started":"2023-06-01T16:04:09.896913Z","shell.execute_reply":"2023-06-01T17:05:56.106795Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8ce536ff5e4df0bf9aa6e15a5c4a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Strings to lists:   0%|          | 0/64 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0a0531f35842f59e4ceab006229bc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with unuequal sequences len:   0%|          | 0/64 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333a30c9aeec472399a0c0408abb0dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Preprocessing normalizedseq2seq:   0%|          | 0/64 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728badcc3eb642a2810fecd71108f6af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Making consistent:   0%|          | 0/63593 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e79beba5f884b5aaed04b55feef03d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing quote types:   0%|          | 0/63593 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f468ab83604e7b9882ad7a69439042"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Excluding types:   0%|          | 0/63593 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b8893786e6484487bc219bf32caada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving type alias:   0%|          | 0/63593 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf0684cb58f43ef80d2d9aa4ad4edcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Reducing parameters:   0%|          | 0/63593 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82cad0c3561143a5b6a691404b4a5d24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing trivial annotations:   0%|          | 0/64 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc978fd1f954fc2aa539fa170b36d44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/64 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8efa1f5ab4b042dbb77b90a264e37c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing for the model:   0%|          | 0/4738 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"253531a35134444897b061f4d42e26a5"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/142 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf4ea2b7d0844eb8ba4eb8248e36cf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Strings to lists:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f4ceee154d54325815a4c792e2d9faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with unuequal sequences len:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb773f1253bb4d118532ea86268b3da1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Preprocessing normalizedseq2seq:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e6e20629a54b0dbd18f88da08a91ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Making consistent:   0%|          | 0/6936 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56c7ec0ac694c939b05dd58704841e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing quote types:   0%|          | 0/6936 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec4ec19251f41139c5406bca70f1710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Excluding types:   0%|          | 0/6936 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd42e3a129584bfca828e1f603199bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving type alias:   0%|          | 0/6936 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7514a7cc40e4720814ef82f9d1f3ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Reducing parameters:   0%|          | 0/6936 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763b0d9463c14a27bd5a21abecfd3fd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing trivial annotations:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d87cee8094844a3b88ada9330447cd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2024fb5c03494c3abb3ce743fc6b6666"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing for the model:   0%|          | 0/520 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5931309b62b64def80f0a5f04f7d0e70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/16 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bb8d88c50f42d2947aa89ab32834fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65061c73ec164b8e83ec40fbb7973450"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b65a4706d0242daa122f972bc3fdb52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/893 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823314128fd54a52a5be718ab82c1a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54793de5cb3944a493dd3ef7d2fdb0bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Strings to lists:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14536c9267754d55b62c3189d511bd93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with unuequal sequences len:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfafedaa457f48f39b1428c9776d5a1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Preprocessing normalizedseq2seq:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da12e6b375c74372b350b3e9c9b9d7ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Making consistent:   0%|          | 0/17616 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75088f5a8b284db6bd0056495e329d72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing quote types:   0%|          | 0/17616 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b46f9315c4a4e7dbeefc7131a9693b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Excluding types:   0%|          | 0/17616 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d1a529f2b94513b091f0e0f2d3a6d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving type alias:   0%|          | 0/17616 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3677a393def40feb3af24d083c904a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Reducing parameters:   0%|          | 0/17616 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88787797f51e401eabdd2471eb37e32e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing trivial annotations:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef8b17823d8441280a9eb09f7896bc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"356f7c1afaa447d29edac894cc7c1b98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing for the model:   0%|          | 0/1313 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d39b233d7e3483d8a2cda2c353b7fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/41 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c3de0c389649deab4b03f431be7d2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8841ba377a0949c3a165a79a79eabc7c"}},"metadata":{}},{"name":"stdout","text":"Overall unique: 40074\ntrain stats: {'all_num': 886306, 'all_unique': 30803, 'ubiquitous_num': 446872, 'ubiquitous_unique': 5, 'common_num': 276017, 'common_unique': 318, 'rare_num': 163417, 'rare_unique': 30480}\nvalid stats: {'all_num': 94545, 'all_unique': 4736, 'ubiquitous_num': 49143, 'ubiquitous_unique': 5, 'common_num': 26735, 'common_unique': 244, 'rare_num': 18667, 'rare_unique': 4487}\ntest stats: {'all_num': 240860, 'all_unique': 10372, 'ubiquitous_num': 121273, 'ubiquitous_unique': 5, 'common_num': 71650, 'common_unique': 270, 'rare_num': 47937, 'rare_unique': 10097}\n","output_type":"stream"}]},{"cell_type":"code","source":"# peforms KNN lookups\nclass FaissKNeighbors:\n    def __init__(self, is_cuda, emb_dim):\n        self.is_cuda = is_cuda\n        self.index = faiss.IndexFlatL2(emb_dim)\n        if self.is_cuda:\n            self.index = faiss.index_cpu_to_all_gpus(self.index)\n\n    def add(self, X):\n        self.index.add(X)\n\n    def predict(self, X, k):\n        distances, indices = self.index.search(X, k=k)\n        return indices\n\n    def reset(self):\n        self.index.reset()\n\n    def __del__(self):\n        self.index.reset()\n\n\n# returns reciprocal ranks\ndef get_rrs(query_labels, pred_labels, k):\n    if k > pred_labels.shape[-1]:\n        raise Exception('k is greater than the number of predictions for each retreival')\n    targets = query_labels.unsqueeze(-1).expand(-1, k)\n    preds = pred_labels[:, :k]\n\n    all_ranks = torch.arange(1, k+1, device=preds.device).unsqueeze(0).expand(preds.shape[0], -1)\n    valid_ranks = torch.where(preds == targets, all_ranks, k+1)\n    rank_per_query = valid_ranks.min(dim=1).values\n    filtered_ranks = rank_per_query[rank_per_query != k+1]\n    reciprocal_ranks = 1 / filtered_ranks\n    reciprocal_ranks = torch.cat((reciprocal_ranks, torch.zeros((preds.shape[0] - reciprocal_ranks.shape[0]), device=preds.device)))\n    return reciprocal_ranks\n\n\n# returns whether there is a heat for each retreival\ndef get_hits(query_labels, pred_labels, k):\n    if k > pred_labels.shape[-1]:\n        raise Exception('k is greater than the number of predictions for each retreival')\n\n    targets = query_labels.unsqueeze(-1).expand((-1, k))\n    preds = pred_labels[:, :k]\n    hits = torch.eq(targets, preds).any(1)\n    return hits\n\n\nclass MetricsCaclulator:\n    def __init__(self, is_cuda, emb_dim=MODEL_PARAMS['emb_dim']):\n        self.faiss = FaissKNeighbors(is_cuda, emb_dim=emb_dim)\n        \n    def calculate_metrics(self, reference, reference_labels,\n                     query, query_labels, k=10, show_progress=True, title=''):\n        self.faiss.add(reference.cpu())\n        metrics = self.calculate_all(reference_labels, query, query_labels, k, show_progress)\n        self.faiss.reset()\n        if title:\n            return {f'{title}_{k}': v for k, v in metrics.items()}\n        else:\n            return metrics\n    \n    # calculates metrics for the entire dataset\n    def calculate_all(self, reference_labels, query, query_labels, k, show_progress):\n        dl = torch.utils.data.DataLoader(list(zip(query, query_labels)), batch_size=FLAGS['batch_size_for_knn'])\n        reciprocal_ranks, top1_hits, top3_hits, top5_hits, top10_hits = [], [], [], [], []\n        for query_batch, query_labels_batch in tqdm(dl, desc='Calculating Metrics', disable=not show_progress):\n            results_of_batch = self.calculate_batch(reference_labels=reference_labels, query=query_batch, query_labels=query_labels_batch, k=k)\n            reciprocal_ranks.append(results_of_batch[0])\n            top1_hits.append(results_of_batch[1])\n            top3_hits.append(results_of_batch[2])\n            top5_hits.append(results_of_batch[3])\n            top10_hits.append(results_of_batch[4])\n        \n        mrr = torch.cat(reciprocal_ranks).mean()\n        hit_rate_top1 = torch.cat(top1_hits).to(torch.float32).mean()\n        hit_rate_top3 = torch.cat(top3_hits).to(torch.float32).mean()\n        hit_rate_top5 = torch.cat(top5_hits).to(torch.float32).mean()\n        hit_rate_top10 = torch.cat(top10_hits).to(torch.float32).mean()\n        \n        del reciprocal_ranks\n        del top1_hits\n        del top3_hits\n        del top5_hits\n        del top10_hits\n        gc.collect()\n        \n        metrics = {\n            f'MRR@{k}': mrr,\n            'hit_rate_top1': hit_rate_top1,\n            'hit_rate_top3': hit_rate_top3,\n            'hit_rate_top5': hit_rate_top5,\n            'hit_rate_top10': hit_rate_top10\n        }\n        return metrics\n\n    # calculate for a batch\n    def calculate_batch(self, reference_labels, query, query_labels, k):\n        # N, K\n        pred_idxs = self.faiss.predict(query.cpu(), k=k).to(query_labels.device)\n        pred_labels = reference_labels.expand(pred_idxs.shape[0], -1).gather(index=pred_idxs, dim=1)\n\n        reciprocal_ranks = get_rrs(query_labels, pred_labels, k)\n        top1_hits = get_hits(query_labels, pred_labels, 1)\n        top3_hits = get_hits(query_labels, pred_labels, 3)\n        top5_hits = get_hits(query_labels, pred_labels, 5)\n        top10_hits = get_hits(query_labels, pred_labels, 10)\n        \n        return reciprocal_ranks, top1_hits, top3_hits, top5_hits, top10_hits\n    \nmetrics_calculator = MetricsCaclulator(is_cuda=True)\n# function for metrics calculation\ncalculate_metrics = metrics_calculator.calculate_metrics","metadata":{"execution":{"iopub.status.busy":"2023-05-31T11:18:03.272290Z","iopub.execute_input":"2023-05-31T11:18:03.272660Z","iopub.status.idle":"2023-05-31T11:18:05.480466Z","shell.execute_reply.started":"2023-05-31T11:18:03.272625Z","shell.execute_reply":"2023-05-31T11:18:05.479492Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Model(pl.LightningModule):\n    def __init__(self, margin, loss_type, token_embeddings_size, learning_rate, batch_size, seed=None):\n        # setting up the model\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.semi_hard_miner = miners.TripletMarginMiner(margin=self.hparams.margin, type_of_triplets='semihard')\n        self.hard_miner = miners.TripletMarginMiner(margin=self.hparams.margin, type_of_triplets='hard')\n        self.loss_fn = losses.TripletMarginLoss(margin=self.hparams.margin)\n        \n        self.train_semi_hard_loss = MeanMetric(nan_strategy='error')\n        self.train_hard_loss = MeanMetric(nan_strategy='error')\n        \n\n        self.model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n        self.model.resize_token_embeddings(token_embeddings_size)\n        for p in self.model.parameters():\n            p.requires_grad = True\n\n\n    def forward(self, input_ids_batch, attention_mask_batch):\n        embs = self.model(input_ids=input_ids_batch, attention_mask=attention_mask_batch).last_hidden_state\n        return embs\n\n\n    def training_step(self, batch, batch_nb):\n        input_ids_batch = batch['input_ids_batch']\n        attention_mask_batch = batch['attention_mask_batch']\n        typed_seq_batch = batch['typed_seq_batch']\n\n        embs = self(input_ids_batch, attention_mask_batch)\n        has_type_annotation_mask = typed_seq_batch != -100\n        embs_with_type_annotation = embs[has_type_annotation_mask]\n        type_ids = typed_seq_batch[has_type_annotation_mask]\n        \n        semi_hard_triplets = self.semi_hard_miner(embs_with_type_annotation, type_ids)\n        hard_triplets = self.hard_miner(embs_with_type_annotation, type_ids)\n        \n        semi_hard_loss = self.loss_fn(embs_with_type_annotation, type_ids, semi_hard_triplets)\n        hard_loss = self.loss_fn(embs_with_type_annotation, type_ids, hard_triplets)\n\n        if self.hparams.loss_type == 'SEMI_HARD':\n            loss = semi_hard_loss\n        else:\n            loss = hard_loss\n\n        \n        self.train_semi_hard_loss(semi_hard_loss)\n        self.train_hard_loss(hard_loss)\n        \n        self.log('train_semi_hard_loss', self.train_semi_hard_loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_hard_loss', self.train_hard_loss, on_step=True, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    # calculating and storing all embeddings\n    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n        input_ids_batch = batch['input_ids_batch']\n        attention_mask_batch = batch['attention_mask_batch']\n        typed_seq_batch = batch['typed_seq_batch']\n        typed_seq_batch_wo_t_params = batch['typed_seq_batch_wo_t_params']\n\n        embs = self(input_ids_batch, attention_mask_batch).detach()\n\n        has_type_annotation_mask = typed_seq_batch != -100\n        embs_with_type_annotation = embs[has_type_annotation_mask]\n        type_ids = typed_seq_batch[has_type_annotation_mask]\n        type_ids_wo_t_params = typed_seq_batch_wo_t_params[has_type_annotation_mask]\n\n        if dataloader_idx == 0:\n            if batch_idx == 0:\n                self.feature_bank = [embs_with_type_annotation.cpu()]\n                self.target_bank = [type_ids.cpu()]\n                self.target_bank_wo_t_params = [type_ids_wo_t_params.cpu()]\n            else:\n                self.feature_bank.append(embs_with_type_annotation.cpu())\n                self.target_bank.append(type_ids.cpu())\n                self.target_bank_wo_t_params.append(type_ids_wo_t_params.cpu())\n        else:\n            if batch_idx == 0:\n                self.query_embeddings = [embs_with_type_annotation.cpu()]\n                self.query_labels = [type_ids.cpu()]\n                self.query_labels_wo_t_params = [type_ids_wo_t_params.cpu()]\n                self.query_positions = [torch.argwhere(has_type_annotation_mask)[:, 1].flatten().cpu()]\n            else:\n                self.query_embeddings.append(embs_with_type_annotation.cpu())\n                self.query_labels.append(type_ids.cpu())\n                self.query_labels_wo_t_params.append(type_ids_wo_t_params.cpu())\n                self.query_positions.append(torch.argwhere(has_type_annotation_mask)[:, 1].flatten().cpu())\n            \n\n            semi_hard_triplets = self.semi_hard_miner(embs_with_type_annotation, type_ids)\n            hard_triplets = self.hard_miner(embs_with_type_annotation, type_ids)\n\n            semi_hard_loss = self.loss_fn(embs_with_type_annotation, type_ids, semi_hard_triplets)\n            hard_loss = self.loss_fn(embs_with_type_annotation, type_ids, hard_triplets)\n\n\n            self.log('valid_semi_hard_loss', semi_hard_loss, prog_bar=True)\n            self.log('valid_hard_loss', hard_loss, prog_bar=True)\n\n    # calculating metrics based on stored embeddings\n    def on_validation_epoch_end(self):\n        self.feature_bank = torch.cat(self.feature_bank)\n        self.target_bank = torch.cat(self.target_bank)\n        self.target_bank_wo_t_params = torch.cat(self.target_bank_wo_t_params)\n        self.query_embeddings = torch.cat(self.query_embeddings)\n        self.query_labels = torch.cat(self.query_labels)\n        self.query_labels_wo_t_params = torch.cat(self.query_labels_wo_t_params)\n        \n        types_count = get_types_count(self.target_bank)\n        ubiquitous_types = UBIQUITOUS_TYPE_IDS\n        \n        is_ubiquitous_mask = get_is_ubiquitous_mask(self.query_labels, ubiquitous_types)\n        ubiquitous_embeddings = self.query_embeddings[is_ubiquitous_mask]\n        ubiquitous_labels = self.query_labels[is_ubiquitous_mask]\n        \n        is_common_mask = get_is_common_mask(self.query_labels, types_count, ubiquitous_types)\n        common_embeddings = self.query_embeddings[is_common_mask]\n        common_labels = self.query_labels[is_common_mask]\n        common_labels_wo_t_params = self.query_labels_wo_t_params[is_common_mask]\n        \n        is_rare_mask = get_is_rare_mask(self.query_labels, types_count)\n        rare_embeddings = self.query_embeddings[is_rare_mask]\n        rare_labels = self.query_labels[is_rare_mask]\n        rare_labels_wo_t_params = self.query_labels_wo_t_params[is_rare_mask]\n        \n        \n        all_metrics = calculate_metrics(\n            query=self.query_embeddings, \n            query_labels=self.query_labels, \n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='valid_all'\n        )\n        all_metrics_wo_t_params = calculate_metrics(\n            query=self.query_embeddings, \n            query_labels=self.query_labels_wo_t_params, \n            reference=self.feature_bank,\n            reference_labels=self.target_bank_wo_t_params,\n            title='valid_all_wo_t_params'\n        )\n        ubiquitos_metrics = calculate_metrics(\n            query=ubiquitous_embeddings,\n            query_labels=ubiquitous_labels,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='valid_ubiquitos'\n        )\n        common_metrics = calculate_metrics(\n            query=common_embeddings,\n            query_labels=common_labels,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='valid_common'\n        )\n        common_metrics_wo_t_params = calculate_metrics(\n            query=common_embeddings,\n            query_labels=common_labels_wo_t_params,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank_wo_t_params,\n            title='valid_common_wo_t_params'\n        )\n        rare_metrics = calculate_metrics(\n            query=rare_embeddings,\n            query_labels=rare_labels,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='valid_rare'\n        )\n        rare_metrics_wo_t_params = calculate_metrics(\n            query=rare_embeddings,\n            query_labels=rare_labels_wo_t_params,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank_wo_t_params,\n            title='valid_rare_wo_t_params'\n        )\n        \n        combined_metrics = all_metrics\n        combined_metrics.update(all_metrics_wo_t_params)\n        combined_metrics.update(ubiquitos_metrics)\n        combined_metrics.update(common_metrics)\n        combined_metrics.update(common_metrics_wo_t_params)\n        combined_metrics.update(rare_metrics)\n        combined_metrics.update(rare_metrics_wo_t_params)\n        \n        self.query_positions = torch.cat(self.query_positions)\n        pos_mrr_metrics = {}\n        for pos in range(MODEL_PARAMS['model_max_length']):\n            pos_embeddings = self.query_embeddings[self.query_positions == pos]\n            pos_labels = self.query_labels[self.query_positions == pos]\n            if len(pos_labels) != 0:\n                pos_metrics = calculate_metrics(\n                    query = pos_embeddings,\n                    query_labels = pos_labels,\n                    reference = self.feature_bank,\n                    reference_labels=self.target_bank,\n                    show_progress=False\n                )\n                pos_mrr_metrics[pos] = pos_metrics['MRR@10'].item()\n        self.logger.experiment.log_curve('validation MRR@10 on pos', x=pos_mrr_metrics.keys(), y=pos_mrr_metrics.values())\n            \n        \n        self.log_dict(\n            combined_metrics,\n            on_step=False,\n            on_epoch=True\n        )\n        \n        del self.feature_bank\n        del self.target_bank\n        del self.target_bank_wo_t_params\n        del self.query_embeddings\n        del self.query_labels\n        del self.query_labels_wo_t_params\n        del self.query_positions\n\n        gc.collect()\n\n\n    def test_step(self, batch, batch_idx, dataloader_idx=0):\n        input_ids_batch = batch['input_ids_batch']\n        attention_mask_batch = batch['attention_mask_batch']\n        typed_seq_batch = batch['typed_seq_batch']\n        typed_seq_batch_wo_t_params = batch['typed_seq_batch_wo_t_params']\n\n        embs = self(input_ids_batch, attention_mask_batch).detach()\n\n        has_type_annotation_mask = typed_seq_batch != -100\n        embs_with_type_annotation = embs[has_type_annotation_mask]\n        type_ids = typed_seq_batch[has_type_annotation_mask]\n        type_ids_wo_t_params = typed_seq_batch_wo_t_params[has_type_annotation_mask]\n\n        if dataloader_idx == 0:\n            if batch_idx == 0:\n                self.feature_bank = [embs_with_type_annotation.cpu()]\n                self.target_bank = [type_ids.cpu()]\n                self.target_bank_wo_t_params = [type_ids_wo_t_params.cpu()]\n            else:\n                self.feature_bank.append(embs_with_type_annotation.cpu())\n                self.target_bank.append(type_ids.cpu())\n                self.target_bank_wo_t_params.append(type_ids_wo_t_params.cpu())\n        else:\n            if batch_idx == 0:\n                self.query_embeddings = [embs_with_type_annotation.cpu()]\n                self.query_labels = [type_ids.cpu()]\n                self.query_labels_wo_t_params = [type_ids_wo_t_params.cpu()]\n                self.query_positions = [torch.argwhere(has_type_annotation_mask)[:, 1].flatten().cpu()]\n            else:\n                self.query_embeddings.append(embs_with_type_annotation.cpu())\n                self.query_labels.append(type_ids.cpu())\n                self.query_labels_wo_t_params.append(type_ids_wo_t_params.cpu())\n                self.query_positions.append(torch.argwhere(has_type_annotation_mask)[:, 1].flatten().cpu())\n            \n\n            semi_hard_triplets = self.semi_hard_miner(embs_with_type_annotation, type_ids)\n            hard_triplets = self.hard_miner(embs_with_type_annotation, type_ids)\n\n            semi_hard_loss = self.loss_fn(embs_with_type_annotation, type_ids, semi_hard_triplets)\n            hard_loss = self.loss_fn(embs_with_type_annotation, type_ids, hard_triplets)\n\n\n            self.log('test_semi_hard_loss', semi_hard_loss, prog_bar=True)\n            self.log('test_hard_loss', hard_loss, prog_bar=True)\n\n    def on_test_epoch_end(self):\n        self.feature_bank = torch.cat(self.feature_bank)\n        self.target_bank = torch.cat(self.target_bank)\n        self.target_bank_wo_t_params = torch.cat(self.target_bank_wo_t_params)\n        self.query_embeddings = torch.cat(self.query_embeddings)\n        self.query_labels = torch.cat(self.query_labels)\n        self.query_labels_wo_t_params = torch.cat(self.query_labels_wo_t_params)\n        \n        types_count = get_types_count(self.target_bank)\n        ubiquitous_types = UBIQUITOUS_TYPE_IDS\n        \n        is_ubiquitous_mask = get_is_ubiquitous_mask(self.query_labels, ubiquitous_types)\n        ubiquitous_embeddings = self.query_embeddings[is_ubiquitous_mask]\n        ubiquitous_labels = self.query_labels[is_ubiquitous_mask]\n        \n        is_common_mask = get_is_common_mask(self.query_labels, types_count, ubiquitous_types)\n        common_embeddings = self.query_embeddings[is_common_mask]\n        common_labels = self.query_labels[is_common_mask]\n        common_labels_wo_t_params = self.query_labels_wo_t_params[is_common_mask]\n        \n        is_rare_mask = get_is_rare_mask(self.query_labels, types_count)\n        rare_embeddings = self.query_embeddings[is_rare_mask]\n        rare_labels = self.query_labels[is_rare_mask]\n        rare_labels_wo_t_params = self.query_labels_wo_t_params[is_rare_mask]\n        \n        \n        all_metrics = calculate_metrics(\n            query=self.query_embeddings, \n            query_labels=self.query_labels, \n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='test_all'\n        )\n        all_metrics_wo_t_params = calculate_metrics(\n            query=self.query_embeddings, \n            query_labels=self.query_labels_wo_t_params, \n            reference=self.feature_bank,\n            reference_labels=self.target_bank_wo_t_params,\n            title='test_all_wo_t_params'\n        )\n        ubiquitos_metrics = calculate_metrics(\n            query=ubiquitous_embeddings,\n            query_labels=ubiquitous_labels,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='test_ubiquitos'\n        )\n        common_metrics = calculate_metrics(\n            query=common_embeddings,\n            query_labels=common_labels,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='test_common'\n        )\n        common_metrics_wo_t_params = calculate_metrics(\n            query=common_embeddings,\n            query_labels=common_labels_wo_t_params,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank_wo_t_params,\n            title='test_common_wo_t_params'\n        )\n        rare_metrics = calculate_metrics(\n            query=rare_embeddings,\n            query_labels=rare_labels,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank,\n            title='test_rare'\n        )\n        rare_metrics_wo_t_params = calculate_metrics(\n            query=rare_embeddings,\n            query_labels=rare_labels_wo_t_params,\n            reference=self.feature_bank,\n            reference_labels=self.target_bank_wo_t_params,\n            title='test_rare_wo_t_params'\n        )\n        \n        combined_metrics = all_metrics\n        combined_metrics.update(all_metrics_wo_t_params)\n        combined_metrics.update(ubiquitos_metrics)\n        combined_metrics.update(common_metrics)\n        combined_metrics.update(common_metrics_wo_t_params)\n        combined_metrics.update(rare_metrics)\n        combined_metrics.update(rare_metrics_wo_t_params)\n        \n        self.query_positions = torch.cat(self.query_positions)\n        pos_mrr_metrics = {}\n        for pos in range(MODEL_PARAMS['model_max_length']):\n            pos_embeddings = self.query_embeddings[self.query_positions == pos]\n            pos_labels = self.query_labels[self.query_positions == pos]\n            if len(pos_labels) != 0:\n                pos_metrics = calculate_metrics(\n                    query = pos_embeddings,\n                    query_labels = pos_labels,\n                    reference = self.feature_bank,\n                    reference_labels=self.target_bank,\n                    show_progress=False\n                )\n                pos_mrr_metrics[pos] = pos_metrics['MRR@10'].item()\n        self.logger.experiment.log_curve('test MRR@10 on pos', x=pos_mrr_metrics.keys(), y=pos_mrr_metrics.values())\n            \n        \n        self.log_dict(\n            combined_metrics,\n            on_step=False,\n            on_epoch=True\n        )\n        \n        del self.feature_bank\n        del self.target_bank\n        del self.target_bank_wo_t_params\n        del self.query_embeddings\n        del self.query_labels\n        del self.query_labels_wo_t_params\n        del self.query_positions\n\n        gc.collect()\n\n\n    def predict_step(self, batch, batch_nb):\n        input_ids_batch = batch['input_ids_batch']\n        attention_mask_batch = batch['attention_mask_batch']\n\n        embs = self(input_ids_batch, attention_mask_batch)\n        return embs\n\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)","metadata":{"_uuid":"2946d574-a1d3-4f98-adaa-bfd152cb2175","_cell_guid":"d6a406f4-c3fa-446a-8bdd-314b92d480b1","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-05-31T11:18:05.507643Z","iopub.execute_input":"2023-05-31T11:18:05.507935Z","iopub.status.idle":"2023-05-31T11:18:05.566482Z","shell.execute_reply.started":"2023-05-31T11:18:05.507903Z","shell.execute_reply":"2023-05-31T11:18:05.565480Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if FLAGS['CHECKPOINT_PATH']:\n    if MODEL_PARAMS['keep_same']:\n        model = Model.load_from_checkpoint(\n            FLAGS['CHECKPOINT_PATH'],\n            SEED=FLAGS['SEED']\n        )\n    else:\n        model = Model.load_from_checkpoint(\n            FLAGS['CHECKPOINT_PATH'], \n            margin=MODEL_PARAMS['margin'],\n            loss_type=MODEL_PARAMS['loss_type'],\n            learning_rate=MODEL_PARAMS['lr'],\n            batch_size=FLAGS['batch_size'],\n            seed=FLAGS['SEED']\n        )\nelse:\n    model = Model(\n        margin=MODEL_PARAMS['margin'],\n        loss_type=MODEL_PARAMS['loss_type'],\n        token_embeddings_size=len(tokenizer),\n        learning_rate=MODEL_PARAMS['lr'],\n        batch_size=FLAGS['batch_size'],\n        seed=FLAGS['SEED']\n    )\n\n\n# which metric to monitor\nMETRIC2MONITOR = 'valid_all_MRR@10'\ncheckpoint_callback = ModelCheckpoint(dirpath=f\"{FLAGS['EXPERIMENT_NAME']}_checkpoints\", save_top_k=1, monitor=METRIC2MONITOR, mode='max')\n\n# setting up the logger\ncomet_logger = CometLogger(\n    api_key=COMET_API_KEY,\n    project_name=FLAGS['PROJECT_NAME'],\n    experiment_name=FLAGS['EXPERIMENT_NAME'],\n)\n\n\n# setting up the trainer\ntrainer = pl.Trainer(\n    accelerator=\"gpu\",\n    callbacks=\n    [\n        checkpoint_callback,\n    ],\n    logger=comet_logger,\n    max_epochs=FLAGS['max_epochs'],\n    log_every_n_steps=40,\n    check_val_every_n_epoch=1,\n    enable_progress_bar=True,\n    num_sanity_val_steps=0\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T11:18:05.568173Z","iopub.execute_input":"2023-05-31T11:18:05.568907Z","iopub.status.idle":"2023-05-31T11:18:48.634801Z","shell.execute_reply.started":"2023-05-31T11:18:05.568873Z","shell.execute_reply":"2023-05-31T11:18:48.633823Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2537309a99f4a90bcaae0b17ecbbc9e"}},"metadata":{}}]},{"cell_type":"code","source":"if FLAGS['stage'] == 'train':\n    trainer.fit(model, dm)\nelif FLAGS['stage'] == 'valid':\n    dm.setup('fit')\n    trainer.validate(model, dm)\nelif FLAGS['stage'] == 'test':\n    dm.setup('test')\n    trainer.test(model, dm)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T11:18:48.636246Z","iopub.execute_input":"2023-05-31T11:18:48.636649Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1adbca1ba80c7406/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6f8715b38e4564a1b9c768824bb922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e083cc364f3648fcae5a17875e6e127c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1adbca1ba80c7406/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb2ca26536cb43a69a07c2588c0aaa27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Strings to lists:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed58b96487b424cb5927db338923497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with unuequal sequences len:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f5aa50d72d641f495fa61b94ef3e3d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Preprocessing normalizedseq2seq:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b64d5b7b79c49bdbf0d646186a1e3c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Making consistent:   0%|          | 0/6362 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9df17b409994cf9a39a901f98edd74a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing quote types:   0%|          | 0/6362 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99c49b7f1abc428da1c7c2fb7b73f08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Excluding types:   0%|          | 0/6362 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f5fb8c23bec4fe091411f40bf38a02a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving type alias:   0%|          | 0/6362 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc3397a03f640518ff15f162fd11abe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Reducing parameters:   0%|          | 0/6362 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25aa7ab5f294900bd671e20f38f06ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing trivial annotations:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4a212807a524df5b0af6f28d2dd6bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11dac82c105e4285b81b594a167d5517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing for the model:   0%|          | 0/480 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf40875139b4f80a894f22a834e57ec"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2f4ad441f744fbab956f545f5f97d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Strings to lists:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d929e6bec4a641ddaa3a672b925d9361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with unuequal sequences len:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6fa8f47479436a9ab996d6c7d9fe2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Preprocessing normalizedseq2seq:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"752d2122d3414fb0a9382071772195ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Making consistent:   0%|          | 0/694 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5438125a57143b5903bd3f77259930f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing quote types:   0%|          | 0/694 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa9f692de384a1dafaf50d01b30bfb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Excluding types:   0%|          | 0/694 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c3b4d6897d4a63ad8646007792f8a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving type alias:   0%|          | 0/694 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87e7d2f11b674b0ba19058ac5c9fdc21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Reducing parameters:   0%|          | 0/694 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b95a6692cf4bf5a489b525b9bdba73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing trivial annotations:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9431028a0bfd45649c1ba9e85b623723"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4835f008a37a44f999b574726eca4f52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing for the model:   0%|          | 0/53 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c339c4de3fb64018bc92d307f68950a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Removing examples with no annotations:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eefa206912fb4fbcb955040f60c493d1"}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: keras, tensorflow, sklearn, torch, tensorboard.\n\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/gfx73/typebert4py/d10fff27c78c4e86aa55e5c1877d9b16\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b152ec28a11d46b59b0a2f0372d8e618"}},"metadata":{}}]},{"cell_type":"code","source":"# # saving previous runs weights\n# import os\n# from pathlib import Path\n# from distutils.dir_util import copy_tree\n# INPUT_DIR = Path('/kaggle/input/codebertdsl-2')\n# OUTPUT_DIR = Path('/kaggle/working')\n# for p in os.listdir(INPUT_DIR):\n#     if '_checkpoints' in p:\n#         source_dir = INPUT_DIR / p\n#         dest_dir = OUTPUT_DIR / p\n#         if not os.path.exists(dest_dir):\n#             os.mkdir(str(dest_dir))\n#             copy_tree(str(source_dir), str(dest_dir))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}